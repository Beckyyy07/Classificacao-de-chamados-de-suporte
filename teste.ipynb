{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7f0caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "# Baixa os recursos necessários do NLTK\n",
    "nltk.download('punkt')        # Tokenizador pré-treinado\n",
    "nltk.download('stopwords')    # Lista de stopwords (palavras irrelevantes) em vários idiomas\n",
    "nltk.download('wordnet')      # Base de dados semântica usada na lematização (em inglês)\n",
    "\n",
    "# Importa componentes do NLTK\n",
    "from nltk.corpus import stopwords  # Lista de palavras irrelevantes para remoção\n",
    "from nltk.stem import SnowballStemmer, RSLPStemmer, PorterStemmer, WordNetLemmatizer  # Stemmers e lematizador\n",
    "from nltk.tokenize import TweetTokenizer  # Tokenizador ideal para textos informais (como tweets, emojis, hashtags)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057a9e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando DF\n",
    "df = pd.read_csv('database/aa_dataset-tickets-multi-lang-5-2-50-version.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a172ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c98132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando idiomas do DF\n",
    "print(df['language'].unique())\n",
    "\n",
    "# Verificando categorias dos chamados\n",
    "print(df['type'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb8901b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrando os emaisl apenas na lingua inglesa\n",
    "df_en = df[df['language'] == \"en\"]\n",
    "\n",
    "# Removendo colunas não utilizadas\n",
    "df_en = df_en.drop(columns= [\"subject\", \"answer\", \"queue\", \"priority\", \"language\", \"version\", \"tag_1\", \"tag_2\", \"tag_3\", \"tag_4\", \"tag_5\", \"tag_6\", \"tag_7\", \"tag_8\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cea329a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando se os dados necessário estão no df\n",
    "df_en.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce498d",
   "metadata": {},
   "source": [
    "# Pré-processamento do texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a16001",
   "metadata": {},
   "source": [
    "### Tokenização"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1390da4",
   "metadata": {},
   "source": [
    "Dividir o texto em unidades menores que podem ser facilmente processados ​​por modelos de PNL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aa5bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizador(data):\n",
    "    # Inicializar a classe tokenizer\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokenized_text = tokenizer.tokenize(data)\n",
    "    \n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8024c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en['tokenized_body'] = df_en['body'].apply(tokenizador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1c1555",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872f6277",
   "metadata": {},
   "source": [
    "### Remoção de stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf87ac4",
   "metadata": {},
   "source": [
    "São consideradas como tendo pouco valor semântico ou são insignificantes para a tarefa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ce3dd",
   "metadata": {},
   "source": [
    "Cada linguagem, tem sua lista de stop words. Como a língua dos textos que estamos pré-processando é inglês, vamos lidar com elas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254fd7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando a lista de stop words em inglês da biblioteca NLTK\n",
    "stopwords_english = stopwords.words('english') \n",
    "\n",
    "# Exemplos\n",
    "print('Stop words - inglês\\n')\n",
    "print(stopwords_english)\n",
    "\n",
    "print('\\nPontuação\\n')\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693a2a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords(data):\n",
    "\n",
    "    text_cleaned = []\n",
    "\n",
    "    for word in data: # analisa cada palavra da lista de tokens\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "            word not in string.punctuation):  # remove pontuação\n",
    "            text_cleaned.append(word)\n",
    "\n",
    "    return text_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09707539",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en['stopword_removed'] = df_en['tokenized_body'].apply(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d17f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195c1237",
   "metadata": {},
   "source": [
    "### Stemming "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6036c6d1",
   "metadata": {},
   "source": [
    "Processo de reduzir a forma flexionada de uma palavra ao “radical” ou forma raiz, também conhecido como “lema” em linguística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5882e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(data):\n",
    "    # Iniciliza a classe stemming\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Cria lista vazia para armazenar os stems\n",
    "    text_stems = []\n",
    "\n",
    "    for word in data:\n",
    "        stem_word = stemmer.stem(word)\n",
    "        text_stems.append(stem_word)\n",
    "        \n",
    "    return text_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb523cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en['stemmed_text'] = df_en['stopword_removed'].apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6855ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62116021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Junta todos os textos em uma única string\n",
    "texto = ' '.join(df_en['stemmed_text'].astype(str))\n",
    "\n",
    "# Cria a nuvem de palavras\n",
    "nuvem = WordCloud(\n",
    "    background_color='white',\n",
    "    stopwords=stopwords_english,\n",
    "    width=800,\n",
    "    height=400,\n",
    "    colormap='viridis'\n",
    ").generate(texto)\n",
    "\n",
    "# Exibe a nuvem\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(nuvem, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Nuvem de Palavras\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa681f90",
   "metadata": {},
   "source": [
    "## Vetorização"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7888e2",
   "metadata": {},
   "source": [
    "### TF- IDF\n",
    "TF-IDF é o produto da Frequência do Termo e da Frequência Inversa do Documento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e9c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converta para string, caso necessário\n",
    "corpus = df_en['body'].astype(str)\n",
    "\n",
    "# Inicializa o vetorizador TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')  # Pode incluir mais params como ngram_range, max_features, etc.\n",
    "X_tfidf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Supondo que sua coluna 'body' já esteja pré-processada e unida como texto\n",
    "corpus = df_en['body'].astype(str)\n",
    "\n",
    "# Criar o vetor TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  # Limite de features, pode ajustar\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# tfidf_matrix é uma matriz esparsa (documentos x palavras)\n",
    "print(\"Shape:\", tfidf_matrix.shape)\n",
    "\n",
    "#  transformar em um DataFrame:\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df_tfidf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

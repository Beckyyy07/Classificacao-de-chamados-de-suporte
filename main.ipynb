{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3737e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     Basic Constraints of CA cert not marked critical\n",
      "[nltk_data]     (_ssl.c:1028)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     Basic Constraints of CA cert not marked critical\n",
      "[nltk_data]     (_ssl.c:1028)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     Basic Constraints of CA cert not marked critical\n",
      "[nltk_data]     (_ssl.c:1028)>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "# Baixa os recursos necessários do NLTK\n",
    "nltk.download('punkt')        # Tokenizador pré-treinado\n",
    "nltk.download('stopwords')    # Lista de stopwords (palavras irrelevantes) em vários idiomas\n",
    "nltk.download('wordnet')      # Base de dados semântica usada na lematização (em inglês)\n",
    "\n",
    "# Importa componentes do NLTK\n",
    "from nltk.corpus import stopwords  # Lista de palavras irrelevantes para remoção\n",
    "from nltk.stem import SnowballStemmer, RSLPStemmer, PorterStemmer, WordNetLemmatizer  # Stemmers e lematizador\n",
    "from nltk.tokenize import TweetTokenizer  # Tokenizador ideal para textos informais (como tweets, emojis, hashtags)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55cbdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('database/aa_dataset-tickets-multi-lang-5-2-50-version.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c514f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3f4e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando quais idiomas estão os emails \n",
    "print(\"Idiomas: \" + ', '.join(df['language'].unique().astype(str)))\n",
    "# (DE- Alemão, EN- Inglês)\n",
    "\n",
    "# Verificando quais categorias existem\n",
    "print(\"Categorias: \" + \", \" .join(df['type'].unique().astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a98b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrando os emaisl apenas na lingua inglesa\n",
    "english_df = df[df['language'] == \"en\"]\n",
    "\n",
    "# Removendo colunas não utilizadas\n",
    "english_df = english_df.drop(columns= [\"answer\", \"queue\", \"priority\", \"language\", \"version\", \"tag_1\", \"tag_2\", \"tag_3\", \"tag_4\", \"tag_5\", \"tag_6\", \"tag_7\", \"tag_8\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9cd032",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab592b59",
   "metadata": {},
   "source": [
    "# Pré-processamento de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d776578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizador e ferramentas\n",
    "tokenizador = TweetTokenizer()\n",
    "stemmer_en = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "\n",
    "def pre_processamento(texto_en):\n",
    "    # Garantir que o texto seja string\n",
    "    texto_en = str(texto_en)\n",
    "\n",
    "    # Tokenização\n",
    "    tokens_en = tokenizador.tokenize(texto_en)\n",
    "\n",
    "    # Tokens sem pontuação\n",
    "    tokens_en_sem_pontuacao = [t for t in tokens_en if t not in string.punctuation]\n",
    "\n",
    "    # Stopwords\n",
    "    tokens_sem_stopwords_en = [t for t in tokens_en_sem_pontuacao if t.lower() not in stopwords_en]\n",
    "\n",
    "    # Stemming\n",
    "    stems_en = [stemmer_en.stem(t) for t in tokens_sem_stopwords_en]\n",
    "\n",
    "    # Lematização\n",
    "    lemmas_en = [lemmatizer.lemmatize(t.lower()) for t in tokens_sem_stopwords_en]\n",
    "\n",
    "    return lemmas_en  # ou tokens_sem_stopwords_en, ou stems_en, conforme o que quiser usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c550fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df['body_processado'] = english_df['body'].apply(pre_processamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc16a66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5774d546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d6f143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Junta todos os textos em uma única string\n",
    "texto = ' '.join(english_df['body_processado'].astype(str))\n",
    "\n",
    "# Cria a nuvem de palavras\n",
    "nuvem = WordCloud(\n",
    "    background_color='white',\n",
    "    stopwords=stopwords_en,\n",
    "    width=800,\n",
    "    height=400,\n",
    "    colormap='viridis'\n",
    ").generate(texto)\n",
    "\n",
    "# Exibe a nuvem\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(nuvem, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Nuvem de Palavras\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a141b357",
   "metadata": {},
   "source": [
    "# Extração de features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2106df22",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "Transforma texto em vetores numéricos com base na frequência relativa das palavras no documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd75b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converta para string, caso necessário\n",
    "corpus = english_df['body'].astype(str)\n",
    "\n",
    "# Inicializa o vetorizador TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')  # Pode incluir mais params como ngram_range, max_features, etc.\n",
    "X_tfidf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Supondo que sua coluna 'body' já esteja pré-processada e unida como texto\n",
    "corpus = english_df['body'].astype(str)\n",
    "\n",
    "# Criar o vetor TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  # Limite de features, pode ajustar\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# tfidf_matrix é uma matriz esparsa (documentos x palavras)\n",
    "print(\"Shape:\", tfidf_matrix.shape)\n",
    "\n",
    "#  transformar em um DataFrame:\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df_tfidf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
